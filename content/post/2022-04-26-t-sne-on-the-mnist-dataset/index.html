---
title: "Clustering: PCA vs t-SNE on the Fashion MNIST dataset"
author: Josh Cheema
date: '2022-04-26'
slug: []
categories:
  - Data Analysis & Visualisation
tags:
  - Classification
  - Dimension Reduction
output:
  html_document:
    fig_caption: yes
---



<div id="principal-component-analysis" class="section level2">
<h2>Principal Component Analysis</h2>
<p>Recently I’ve been working on projects involving high-dimensional datasets with hundreds or thousands of variables, which naturally led me to dimension reduction techniques to better visualise and model the data (e.g. cluster analysis). The first port of call for most people will be Principal Component Analysis (“PCA”). In simple terms, PCA determines the directions (principal components) in which the data varies the most by decomposing the sample covariance matrix, <span class="math inline">\(S\)</span>, into its eigenvectors and eigenvalues. For <span class="math inline">\(j=1,..,p\)</span>, the <span class="math inline">\(j\)</span>-th principal component is defined to be the eigenvector associated with the <span class="math inline">\(j\)</span>-th largest eigenvalue of <span class="math inline">\(S\)</span>, with principal components being orthogonal to each other. The principal components are therefore arranged in order of the amount of variance explained and we can visualise the number of principal components required to explain a significant level of variance in the data. From a human perspective, this is most helpful if a large portion of the variance can be explained by 2 or 3 principal components, allowing us to easily visualise the data and any patterns. However, as long as the number of principal components required is less than the number of dimensions of the original dataset the application of machine learning algorithms becomes less computationally expensive. The orthogonality of the principal components also helps avoid issues with multicollinearity in the dataset. In addition, given the principal components will be linear combinations of variables in the original dataset, we also perform a form of feature selection as the principal components explaining the majority of the variance will assign a higher weight to the original variables that explain the majority of the variation in the original dataset.</p>
<p>To illustrate PCA in action, we will use a subset of the Fashion MNIST dataset which is a collection of digital images of fashion items. Each image is comprised of 784 pixels (arranged in 28 <span class="math inline">\(\times\)</span> 28 pixel arrays) with each pixel corresponding to a grayscale integer value between 0 and 255, where 0 is white and 255 is black. We will be loading a subset (2000 observations) of the MNIST data which can be downloaded from <a href="https://www.kaggle.com/datasets/zalando-research/fashionmnist">Kaggle</a>. The dataset also includes a numeric ‘label’ variable and we will add a further ‘item’ variable to include a text description of the item type. We can therefore consider our dataframe with pixel data and a single label variable as having <span class="math inline">\((n \times p)\)</span> dimensions where <span class="math inline">\(n = 2000\)</span> and <span class="math inline">\(p=785\)</span>.</p>
<pre class="r"><code>library(tidyverse)

# Load Fashion MNIST training dataset downloaded 
# from https://www.kaggle.com/datasets/zalando-research/fashionmnist

fashion_mnist &lt;- read_csv(&quot;~/Datasets/Fashion_MNIST/fashion-mnist_train.csv&quot;)

# We will take 2000 observations from the &#39;train&#39; data

set.seed(123)
sample_id &lt;- sample(nrow(fashion_mnist), 2000)

# Select samples from original dataset

fashion_mnist &lt;- fashion_mnist[sample_id,]

# Set labels as factor type

fashion_mnist$label &lt;- as.factor(fashion_mnist$label)

# Convert pixel values based on scale between 0 and 1

fashion_mnist[,2:ncol(fashion_mnist)] &lt;- fashion_mnist[,2:ncol(fashion_mnist)]/255

# Add Item variable with description of item type

fashion_mnist &lt;- fashion_mnist %&gt;%
  mutate(Item = case_when(
    label == 0 ~ &quot;T-shirt&quot;,
    label == 1 ~ &quot;Trouser&quot;,
    label == 2 ~ &quot;Pullover&quot;,
    label == 3 ~ &quot;Dress&quot;,
    label == 4 ~ &quot;Coat&quot;,
    label == 5 ~ &quot;Sandal&quot;,
    label == 6 ~ &quot;Shirt&quot;,
    label == 7 ~ &quot;Sneaker&quot;,
    label == 8 ~ &quot;Bag&quot;,
    label == 9 ~ &quot;Boot&quot;))

# Define a function to visualise the images

view &lt;- function(data){
  view_df &lt;- data[1:9,] %&gt;% # Select first 9 entries as an example
    mutate(img_id = row_number()) %&gt;% # Create image id variable to track order in dataset
    pivot_longer(cols = starts_with(&quot;pixel&quot;), # Pivot table longer based on pixel values
                 names_to = &quot;pixel&quot;, 
                 names_prefix = &quot;pixel&quot;, 
                 values_to = &quot;value&quot;) %&gt;%
    arrange(pixel) %&gt;% # Sort pixels in order so pixel numbers are grouped rather than label/item
    mutate(pixel = as.numeric(pixel) - 1, # Add x and y variables to show location of pixel value in 2D space
           x = pixel %% 28,
           y = 28 - (pixel %/% 28))

  ggplot(view_df, aes(x = x, y = y, fill = value)) +
    geom_tile() + # geom_tile for visualisation
    facet_wrap(vars(img_id, Item), labeller = &quot;label_both&quot;) + # facet on image id and Item type
    scale_fill_gradient(low = &quot;white&quot;, high = &quot;black&quot;) + # use white to black colour gradient
    coord_fixed() + # fix ratio between units on x and y axes
    theme(legend.position = &quot;none&quot;) # remove legend
}

view(fashion_mnist) # visualise selection of original images</code></pre>
<div class="figure"><span style="display:block;" id="fig:load-MNIST-data"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/load-MNIST-data-1.png" alt="Visualisation of Fashion MNIST images without dimension reduction" width="672" />
<p class="caption">
Figure 1: Visualisation of Fashion MNIST images without dimension reduction
</p>
</div>
<p>In Figure <a href="#fig:load-MNIST-data">1</a> above, we see the visualisation of 9 images from the Fashion MNIST sample dataset involving all 784 pixel variables that gives us a clear picture of the original items. We now apply PCA to see if we can get a similarly good picture for these items using a fraction of the number of dimensions (i.e. significantly lower than the 784 pixel variables used to generate Figure <a href="#fig:load-MNIST-data">1</a>).</p>
<p>In R, we will use the <code>prcomp</code> function which applies singular value decomposition to calculate the eigenvalues and eigenvectors of the covariance matrix, <span class="math inline">\(S\)</span>.</p>
<pre class="r"><code># Perform PCA analysis on numeric data (after removing label and Item variables)
fashion_pca &lt;- prcomp(fashion_mnist %&gt;% select(-label, -Item))

# Calculate total variance for all principal components
total_variance &lt;- sum(fashion_pca$sdev^2)

# Calculate cumulative sum of variance as each principal component is added
pc_variance &lt;- cumsum(fashion_pca$sdev^2)

# Ratio of variance explained by n principal components to total variance
variance_explained &lt;- pc_variance/total_variance

# Create dataframe for visualisation
pc_df &lt;- data.frame(PC_num = 1:length(variance_explained), variance_explained)

# Visualise variance explained
ggplot(pc_df, aes(x = PC_num, y = variance_explained)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = seq.int(0, max(pc_df$PC_num), by = 100)) +
  scale_y_continuous(breaks = seq(0,1,by = 0.1))</code></pre>
<div class="figure"><span style="display:block;" id="fig:pca-analysis"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/pca-analysis-1.png" alt="Fashion MNIST: Variance explained vs number of principal components" width="672" />
<p class="caption">
Figure 2: Fashion MNIST: Variance explained vs number of principal components
</p>
</div>
<p>Figure <a href="#fig:pca-analysis">2</a> shows the cumulative percentage of variance that is explained by the first <span class="math inline">\(n\)</span> principal components (i.e. how much of the variance in the original dataset is explained by the first <span class="math inline">\(n\)</span> principal compoenents). Based on Figure <a href="#fig:pca-analysis">2</a>, we can see that approximately 95% of the variance is explained by the first 200 principal components and approximately 100% of the variance is explained by the first 400 principal components. Therefore, compared to the 784 variables in the original dataset, we can reduce the number of variables by slightly less than 75% and still explain 95% of the variance of the data!</p>
<p>To illustrate that the vast majority of the variance is still explained by the first 200 principal components, Figure <a href="#fig:pca-visualisation">3</a> shows the same visualisation as Figure <a href="#fig:load-MNIST-data">1</a> using only the first 200 principal components rather than all 784 variables in the original dataset. In order to create the 784 values for each pixel in Figure <a href="#fig:pca-visualisation">3</a>, we need to approximate the original data from the principal components based on the below formula:</p>
<p><span class="math display">\[ X \approx \bar{X} + (X - \bar{X})UU^{T}  \]</span></p>
<p>where <span class="math inline">\(\bar{X}\)</span> is the <span class="math inline">\((n \times p)\)</span> matrix of column means for the original pixel data <span class="math inline">\(X\)</span> and <span class="math inline">\(U\)</span> is the <span class="math inline">\((p \times r)\)</span> matrix of the top <span class="math inline">\(r\)</span> principal components (in this case <span class="math inline">\(r = 200\)</span>).</p>
<pre class="r"><code># Choose number of principal components
num_pc &lt;- 200

# Extract U as rotation matrix from PCA analysis
mnist_U &lt;- matrix(fashion_pca$rotation[,1:num_pc], ncol = num_pc)

# Extract XU as principal components from PCA analysis
mnist_XU &lt;- matrix(fashion_pca$x[,1:num_pc], ncol = num_pc)

# Calculate column means of X
xbar &lt;- colMeans(fashion_mnist %&gt;% select(-label, -Item))

# Create matrix of column means for each observation
xbar &lt;- matrix(rep(xbar, 2000), byrow = TRUE, nrow = 2000)

# Reconstruct original data
X_recon &lt;- data.frame(pixel = matrix(xbar + (mnist_XU %*% t(mnist_U)), ncol = 784))

# Tidy to replace periods with spaces
names(X_recon) &lt;- gsub(x = names(X_recon),
                       pattern = &quot;\\.&quot;,
                       replacement = &quot; &quot;)

# Combine into data frame for visualisation
mnist_pc &lt;- bind_cols(Item = fashion_mnist$Item, pixel = X_recon)

view(mnist_pc) # View reconstructed PCA data</code></pre>
<div class="figure"><span style="display:block;" id="fig:pca-visualisation"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/pca-visualisation-1.png" alt="Visualisation of Fashion MNIST images with 200 principal components" width="672" />
<p class="caption">
Figure 3: Visualisation of Fashion MNIST images with 200 principal components
</p>
</div>
<p>As we can see, the approximated images are very good representations of the originals, which is to be expected given that the 200 principal components explain roughly 95% of the variance in the original pixel data.</p>
<p>We can also use PCA to visualise the original high-dimensional data in two dimensions to see if any clusters are clearly visible, for example, which helps give an indication of how classification algorithms might perform on the two-dimensional data.</p>
<pre class="r"><code>pc_1 &lt;- fashion_pca$x[,1] # Extract first principal component
pc_2 &lt;- fashion_pca$x[,2] # Extract second principal component

# Combine into data frame for visualisation
pca_2d &lt;- data.frame(Item = as.factor(fashion_mnist$Item), pc_1, pc_2)

# Visualise as scatter plot
ggplot(pca_2d, aes(x = pc_1, y = pc_2, col = Item)) +
  geom_point(size = 1)</code></pre>
<div class="figure"><span style="display:block;" id="fig:pca-2d"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/pca-2d-1.png" alt="Projection of original Fashion MNIST data along first two principal components" width="672" />
<p class="caption">
Figure 4: Projection of original Fashion MNIST data along first two principal components
</p>
</div>
<p>In Figure <a href="#fig:pca-2d">4</a>, we see some separation between images of Trousers and the rest of the dataset, and separation of a combined group of Sneakers &amp; Sandals and Boots &amp; Bags versus the rest of the dataset. However, it there is no clear separation within these combined clusters or between other item types in the wider dataset.</p>
<p>Let’s see if the t-SNE analysis can produce better clusters.</p>
</div>
<div id="t-sne" class="section level2">
<h2>t-SNE</h2>
<p>t-SNE or t-distributed stochastic neighbour embedding is a method introduced by (Van der Maaten &amp; Hinton, 2008). t-SNE aims to preserve similarity measures between high-dimensional and low-dimensional space by treating the probability of observations being close together as a random event subject to a probability distribution in high-dimensional and low-dimensional space, respectively, and then minimising the Kullback-Leibler divergence between the two distributions. Therefore, the similarity of observations in high-dimensional space is preserved when projecting observations into a lower dimension.</p>
<p>Mathematically, the probability of point <span class="math inline">\(\mathbf{x}_{j}\)</span> being chosen as a neighbour to point <span class="math inline">\(\mathbf{x}_{i}\)</span> given that <span class="math inline">\(\mathbf{x}_{i}\)</span> is fixed is modelled using a Gaussian kernel:</p>
<p><span class="math display">\[ p_{j|i} = \frac{\frac{1}{\sigma_i \sqrt{2\pi}}e^\frac{-d_{ij}^2}{2\sigma_{i}^2}}{\frac{1}{\sigma_{i}\sqrt{2\pi}}e^\frac{-d_{i1}^2}{2\sigma_{i}^2} +...+ \frac{1}{\sigma_{i}\sqrt{2\pi}}e^\frac{-d_{ik}^2}{2\sigma_{i}^2}} \]</span></p>
<p>where <span class="math inline">\(d_{ij} = ||\mathbf{x}_{i} - \mathbf{x}_{j}||_{2}\)</span> and <span class="math inline">\(p_{j|i}\)</span> simplifies to:</p>
<p><span class="math display">\[ p_{j|i} = \frac{\frac{1}{\sigma_i \sqrt{2\pi}}e^\frac{-d_{ij}^2}{2\sigma_{i}^2}}{\frac{1}{\sigma_{i}\sqrt{2\pi}}\sum_{k\neq i} e^\frac{-d_{ik}^2}{2\sigma_{i}^2}} = \frac{e^{\frac{-d_{ij}^2}{2\sigma_{i}^2}}}{\sum_{k \neq i} e^{\frac{-d_{ik}^2}{2\sigma_{i}^2}}} \]</span></p>
<p>Note: technically we are not limited to a Gaussian kernel and we could also model using a uniform distribution, for example.</p>
<p>Effectively, the probability of point <span class="math inline">\(\mathbf{x}_{j}\)</span> being chosen as a neighbour given we are at point <span class="math inline">\(\mathbf{x}_{i}\)</span> is equivalent to the probability of <span class="math inline">\(\mathbf{x}_{j}\)</span> being selected based on a normal distribution centred at <span class="math inline">\(\mathbf{x}_{i}\)</span> divided by the sum of the probability of all points being selected as the neighbour of <span class="math inline">\(\mathbf{x}_{i}\)</span> based on the same normal distribution. Given that <span class="math inline">\(d_{ij}\)</span> is the Euclidean distance between points <span class="math inline">\(\mathbf{x}_{i}\)</span> and <span class="math inline">\(\mathbf{x}_{j}\)</span>, this results in points that are closer together having a higher probability of being chosen as neighbours, relative to all other points being considered.</p>
<p>Given that the standard deviation <span class="math inline">\(\sigma_{i}\)</span> varies with <span class="math inline">\(i\)</span>, <span class="math inline">\(p_{j|i}\)</span> is an asymmetric similarity measure (i.e. <span class="math inline">\(p_{j|i} \neq p_{i|j}\)</span>). The <em>perplexity</em> of the distribution is a hyperparameter that is chosen at initialisation of the t-SNE algorithm and, roughly speaking, can be thought of as the number of neighbours each point is expected to have. More formally, it is related to the entropy of the distribution which, in turn, is a function of the standard deviation <span class="math inline">\(\sigma_{i}\)</span> for each point <span class="math inline">\(\mathbf{x_{i}}\)</span>. As such, for a given perplexity value, <span class="math inline">\(\sigma_{i}\)</span> is larger for points in regions where the density of data points is lower and vice versa.</p>
<p>This can be shown visually using <a href="#fig:mvt-norm">5</a>, where we see two clearly separated clusters containing 1 and 2 observations, respectively. Considering the orange point in the “circle” cluster and the purple point in the “triangle” cluster, it is obvious that there will be significant differences in the Euclidean distance between the point in the triangle cluster and a point in the circle cluster and the Euclidean distance between both points in the circle cluster.</p>
<pre class="r"><code>library(mvtnorm)

# Generate data points from multivariate normal distributions

set.seed(123)
cluster1 &lt;- data.frame(rmvnorm(2, mean = rep(5, 2), sigma = 4*diag(2)))

set.seed(123)
cluster2 &lt;- data.frame(rmvnorm(1, mean = rep(40, 2), sigma = diag(2)))

# Visualise as scatter plot

ggplot() +
  geom_point(data = cluster1, aes(x=X1, y=X2), shape = 21, fill = &quot;orange&quot;, size = 2) +
  geom_point(data = cluster2, aes(x=X1, y=X2), shape = 24, fill = &quot;purple&quot;, size = 2) +
  xlim(0, 50) +
  ylim(0, 50) +
  labs(x = &quot;X&quot;, y = &quot;Y&quot;)</code></pre>
<div class="figure"><span style="display:block;" id="fig:mvt-norm"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/mvt-norm-1.png" alt="Example of asymmetric similarity measure" width="672" />
<p class="caption">
Figure 5: Example of asymmetric similarity measure
</p>
</div>
<p>From the triangle point’s perspective (i.e. point <span class="math inline">\(\mathbf{x}_{i}\)</span>), <span class="math inline">\(p_{j|i}\)</span> will show the nearest point in the circle cluster as being the most likely neighbour (let’s assume <span class="math inline">\(p_{j|i} = 0.6\)</span> in this case). However, for that same point within the circle cluster (i.e. point <span class="math inline">\(\mathbf{x}_{j}\)</span>) the most likely neighbour will be the other point within the circle cluster and, given the large distance between the circle and triangle clusters, the probability of selecting point <span class="math inline">\(\mathbf{x}_{i}\)</span> as its neighbour will be much smaller (let’s say <span class="math inline">\(p_{i|j} = 0.1\)</span>, for example). Therefore, <span class="math inline">\(p_{j|i} \neq p_{i|j}\)</span> and we need to create a symmetric similarity measure by defining:</p>
<p><span class="math display">\[ p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N} \]</span></p>
<p>where the <span class="math inline">\(2N\)</span> is a scaling factor based on the total number of points <span class="math inline">\(N\)</span> to ensure that <span class="math inline">\(\sum_{i,j} p_{ij} = 1\)</span>.</p>
<p>We then also define the probability of points <span class="math inline">\(\mathbf{y}_{i}\)</span> and <span class="math inline">\(\mathbf{y}_{j}\)</span> being selected as neighbours in the lower dimensional space as:</p>
<p><span class="math display">\[ q_{ij} = \frac{(1 + ||\mathbf{y}_{i} - \mathbf{y}_{j}||_{2}^{2})^{-1}}{\sum_{k \neq l} (1 + ||\mathbf{y}_{k} - \mathbf{y}_{l}||_{2}^{2})^{-1}} \]</span></p>
<p>where <span class="math inline">\(q_{ij}\)</span> is a heavy-tailed Student t-distribution with one degree of freedom, which is equivalent to a Cauchy distribution.</p>
<p>The locations of points <span class="math inline">\(\mathbf{y}_{i}\)</span> are then determined by minimising the Kullback-Leibler divergence of distribution <span class="math inline">\(P(\mathbf{x})\)</span> and <span class="math inline">\(Q(\mathbf{y})\)</span>:</p>
<p><span class="math display">\[ KL (P || Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}\]</span></p>
<p>The minimisation is achieved via gradient descent according to the following steps:</p>
<ol style="list-style-type: decimal">
<li>Calculate the symmetric similarities <span class="math inline">\(p_{ij}\)</span> for all pairs of points <span class="math inline">\(\mathbf{x}_{i}\)</span> and <span class="math inline">\(\mathbf{x}_{j}\)</span></li>
<li>Choose a random solution for the location of each <span class="math inline">\(\mathbf{y}_{i}\)</span></li>
<li>Calculate the similarities <span class="math inline">\(q_{ij}\)</span> for all pairs of points <span class="math inline">\(\mathbf{y}_{i}\)</span> and <span class="math inline">\(\mathbf{y}_{j}\)</span></li>
<li>Calculate the gradient of the cost function (KL divergence)</li>
<li>Update the solution for the location of each <span class="math inline">\(\mathbf{y}_{i}\)</span></li>
<li>Repeat step 4 and 5 until convergence</li>
</ol>
<p>Note: In practice we would set a maximum number of iterations such that the repetition of steps 4 and 5 would stop once the maximum number of iterations is reached (at which point convergence will have hopefully occurred or the maximum number of iterations can be increased).</p>
<p>We can carry out t-SNE analysis in R using the <code>Rtsne</code> package. Here we choose a perplexity value of 35 as it produces a reasonably good visualisation but we could use cross-validation to select the optimum value.</p>
<pre class="r"><code>library(Rtsne)

# Perform t-SNE analysis
set.seed(123)
mnist_tsne &lt;- Rtsne(fashion_mnist %&gt;% select(-label, -Item), 
                    dims = 2, 
                    perplexity = 35, 
                    verbose = FALSE, 
                    max_iter = 5000)

# Extract dimension components from t-SNE analysis for 2D visualisation
mnist_tsne_2d &lt;- data.frame(Item = fashion_mnist$Item, 
                            x = mnist_tsne$Y[,1],
                            y = mnist_tsne$Y[,2])

# Visualise as scatter plot
ggplot(mnist_tsne_2d, aes(x = x, y = y, col = Item)) +
  geom_point(size = 1)</code></pre>
<div class="figure"><span style="display:block;" id="fig:tsne-vis"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/tsne-vis-1.png" alt="Two-dimensional projection of Fashion MNIST data using t-SNE analysis" width="672" />
<p class="caption">
Figure 6: Two-dimensional projection of Fashion MNIST data using t-SNE analysis
</p>
</div>
<p>In Figure <a href="#fig:tsne-vis">6</a> we see a very clear separation of Bag items from Boots, compared to Figure <a href="#fig:pca-2d">4</a>, and the footwear items (Sneakers, Sandals and Boots) are located close together with Boots clearly separated from the other items and better separation between Sneakers and Sandals. In addition we see better clustering of Trousers, Dresses and T-shirts relative to other items of clothing. However, we do not see much improvement in the separation of Coats, Pullovers and Shirts which is to be expected given they are all items worn on the torso with long sleeves. Despite the lack of improvement in this particular area, it is clear that the clusters in Figure <a href="#fig:tsne-vis">6</a> are much better defined than in Figure <a href="#fig:pca-2d">4</a> and we would expect a non-linear classification model (e.g. Quadratic Discriminant Analysis) fitted to the two components produced by our t-SNE analysis to yield a higher degree of accuracy.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Wickham et al., (2019). <em>Welcome to the tidyverse</em>. Journal of Open Source Software, 4(43), 1686, <a href="https://doi.org/10.21105/joss.01686" class="uri">https://doi.org/10.21105/joss.01686</a>.</p>
<p>Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, Torsten Hothorn (2021). <em>mvtnorm: Multivariate Normal and t Distributions</em>. R package version 1.1-3. URL <a href="http://CRAN.R-project.org/package=mvtnorm" class="uri">http://CRAN.R-project.org/package=mvtnorm</a>.</p>
<p>Alan Genz, Frank Bretz (2009). <em>Computation of Multivariate Normal and t Probabilities</em>. Lecture Notes in Statistics, Vol. 195., Springer-Verlag, Heidelberg. ISBN 978-3-642-01688-2.</p>
<p>L.J.P. van der Maaten and G.E. Hinton (2008). <em>Visualizing High-Dimensional Data Using t-SNE</em>. Journal of Machine Learning Research 9(Nov):2579-2605.</p>
<p>L.J.P. van der Maaten (2014). <em>Accelerating t-SNE using Tree-Based Algorithms</em>. Journal of Machine Learning Research 15(Oct):3221-3245.</p>
<p>Jesse H. Krijthe (2015). <em>Rtsne: T-Distributed Stochastic Neighbor Embedding using a Barnes-Hut Implementation</em>, URL: <a href="https://github.com/jkrijthe/Rtsne" class="uri">https://github.com/jkrijthe/Rtsne</a>.</p>
</div>
